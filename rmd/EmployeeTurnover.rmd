---
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(utils)
library(dplyr)
library(knitr)
library(randomForest)
library(caret)
```
# Multiple Linear Regression Model
</br>
### Introduction
<p>We will train a RandomForest on 70% of the sample data utilizing all the predictors in the dataset. 'Attrition' is the response variable which is a Yes/No indicator. The number of Trees we will train is 500 with the square root of the total number of predicators to use with each Tree. Each Tree will use different set of predicators. 
Figure 1: Error and Number of Trees. The more Trees you have the less Error.</p>
```{r}
#Data import and cleaning
dataFileCsv <- "../data/CaseStudy2data.csv"
hr_employee_attrition <- read.csv(dataFileCsv)
attach(hr_employee_attrition)
nzv <- nearZeroVar(hr_employee_attrition)
uniq <- paste(shQuote(names(hr_employee_attrition)[nzv], type="cmd"), collapse=", ")
cat( sprintf("Remove near zero value columns: %s\n", uniq) )
hr_reduced <- hr_employee_attrition[,-nzv]
detach(hr_employee_attrition)
names(hr_reduced)[1] <- "Age"
```
<br>
###Variable Setup
<p>
samplesize - 70% of count of rows in dataset.
datasize - number of records in dataset.
train - number of records in training dataset.
features - number of explanatory variables in dataset.</p>
<br>
###Fitting the RandomForest
<p>All features will be used in this RandomForest. 
70% of dataset is used for training and remaining 30% is used to test the Trees. Number of variables selected are 4.
Figure 1 shows the error is reduced as you add more trees.</p>
```{r}
attach(hr_reduced)

# Data Partition
set.seed(402)
ind <- sample(2, nrow(hr_reduced), replace = TRUE, prob = c(0.7, 0.3))
train <- hr_reduced[ind==1,]
test <- hr_reduced[ind==2,]
samplesize <- (nrow(hr_reduced))*.70
datasize <- nrow(hr_reduced)
features <- as.numeric(ncol(hr_reduced)-1)

hr_reduced.rf <- randomForest(Attrition ~ . , data = hr_reduced , subset = train)
hr_reduced.rf

#PFigure 1
plot(hr_reduced.rf, main = "RandomForest - Error vs Trees Chart")     
```
<br>
###Out of Bag Sample Errors and Error on Test dataset
<p>`r sqrt(features)` random features are selected at each split. 500 trees are tested  `r features` times for all `r features` features.</p>
```{r}

#Random Forest - Classification
library(randomForest)
set.seed(222)
rf <- randomForest(Attrition~., data=train,
                   ntree = 300,
                   mtry = 10,
                   importance = TRUE,
                   proximity = TRUE)
print(rf)
attributes(rf)

# Prediction & Confusion Matrix - train data
library(caret)
p1 <- predict(rf, train)
confusionMatrix(p1, train$Attrition)

#  Prediction & Confusion Matrix - test data
p2 <- predict(rf, test)
confusionMatrix(p2, test$Attrition)

#Error Rate of Random Forest
plot(rf)

# No. of nodes for the trees
hist(treesize(rf),
     main = "No. of Nodes for the Trees",
     col = "Blue")
# Tune mtry
t <- tuneRF(train[,-2], train[,2],
       stepFactor = 0.5,
       plot = TRUE,
       ntreeTry = 300,
       trace = TRUE,
       improve = 0.05)

# Partial Dependence Plot
partialPlot(rf, train, Attrition, "Yes")

# Extract Single Tree
getTree(rf, 150, labelVar = TRUE)

# Multi-dimensional Scaling Plot of Proximity Matrix
MDSplot(rf, train$Attrition)

#Decision tree with party
library(party)
mytree <- ctree(Attrition~., hr_reduced, controls=ctree_control(mincriterion=0.9, minsplit=50))
print(mytree)
plot(mytree,type="simple")

#Misclassification error
tab<-table(predict(mytree), mydata$Attrition)
print(tab)
1-sum(diag(tab))/sum(tab)

#Random Forest - Regression
#oob.err=double(features)
#test.err = double(features)

#mtry is no of Variables randomly chosen at each split
#for(mtry in 1:features)
  
#{
  #rf=randomForest(Attrition ~ . , data = hr_reduced , subset = train, mtry=mtry,ntree=500) 
 # oob.err[mtry] = rf$mse[500] #Error of all Trees fitted
  
  #pred<-predict(rf,hr_reduced[-train,]) #Predictions on Test Set for each Tree
  #test.err[mtry]= with(hr_reduced[-train,], mean( (Attrition - pred)^2)) #Mean Squared Test Error
  
  #cat(mtry," ") #printing the output to the console
#}
```
<br>

####Error on Test Dataset
```{r}
#test.err
```
####Out of Bag Error Estimation
```{r}
#oob.err
```
<br>
<p>
####Plot of Test Error and Out of Bag Error
The red line of Figure 2 show the Out of Bag Error Estimates and the Blue line is the error calculated on Test dataset.
</p>
```{r}
#matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split")
#legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))
```

```{r}
hr_reduced.lm <- glm(relevel(as.factor(Attrition),ref="Yes") ~ relevel(as.factor(JobRole),ref="Research Scientist") + relevel(as.factor(OverTime),ref="Yes") + DistanceFromHome, data = hr_reduced, family =binomial())

summary(hr_reduced.lm)

df.res <- resid(hr_reduced.lm)
df.res
plot(df.res, ylab = "Residuals", main = "Residuals - Age ~ JobRole + OverTime + DistanceFromHome")


detach(hr_reduced)
detach(hr_reduced.rf)
```

